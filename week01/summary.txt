Summary

In “50 Years of Data Science”, David Donoho delineates the difference between being a statistician and a data scientist in the first few pages of the article. It elucidates that statics and mathematics are merely a part of Data Science and not its alternative or an alias. According to the author, the concept of studying and analyzing Big Data was on the run for almost 200 years. The work carried out earlier was based on manual techniques, unlike these days where advanced computing and visual tools are used by data scientists for big data processing to scrutinize, quantify and infer the information hidden in large datasets. With bigger size, comes the bigger process. The skills required to handle big data may involve managing a multi-processor system, where a simple algorithm running on a single processor becomes really complicated to execute. The networked systems need special database handling skillsets and knowledge of cloud computing to get accurate results. Such mastery cannot be expected from a fresher as it takes years of experience to learn all different aspects of production system deployment, says Yanir Seroussi. 

John Tuckey’s vision was coined in the form of the phrase, data analysis. He emphasized that statistical jewel, new computer technology, larger data bodies and quantification in every domain will lay the foundation for this new science. Chambers, the developer of S, realized the potential of data science far more than what Tuckey had proposed for data analysis. In his book “Statistical Modeling: The Two Cultures, 2001” Breiman described two approaches to work on data: generative modeling and predictive modeling where the former is a more traditional way while the later focuses on making predictions and performing prescriptive analysis. Later, Mark Liberman introduced the modern term “Common Task Framework” describing a method to solve data science problems via predictive analysis. It contains three major parts: public data collection, a mass of competitors modeling the data and referee looking for the best model submission made by the participants. To continually check the advancement of this field, getting used to CTF is a good practice for a data science practitioner.

The article successfully briefed six divisions of data science: data gathering, preparation, and exploration; data representation and transformation; computing with data; data modeling; data visualization and presentation; science about data science. The paramount information mentioned in the writing explains two crucial areas in which a data scientist can develop skills to be proficient. First, Modern Databases, one can know RDMS languages (SQL, NoSQL), advanced excel, structures, transformations and every other aspect related to representation and managing the database. Second, Mathematical Representation, this includes diving deep into algorithms and mathematical models responsible for data modeling; analyzing multi-format data; concepts of deep learning is considered an important area of data science.


